{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8cd47f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizerFast,AutoModel,NezhaModel,AutoTokenizer\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "from tqdm import  tqdm\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch.nn as nn\n",
    "from roformer import RoFormerModel\n",
    "# from gau_alpha import GAUAlphaModel, GAUAlphaTokenizer,GAUAlphaTokenizerFast\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"True\"\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7493b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "ENT_CLS_NUM = 9\n",
    "eval_file = '../input/data_label_kfold_v1.1.json'\n",
    "typeid_path = '../input/type2id.json'\n",
    "batchsize = 32\n",
    "ner_maxlen= 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b9ab1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "type2id = json.load(open(typeid_path))\n",
    "type2id = {label: (id-1) for label, id in type2id.items() if label !=\"NA\"}\n",
    "id2type = {}\n",
    "for k, v in type2id.items(): id2type[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "079909de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG1:\n",
    "    num_workers=8\n",
    "    path=\"/data/user0731/yk/SereTOD/code/ef_roberta_add_dev/\"\n",
    "    model=\"/data/user0731/yk/SereTOD/pretrain/roformer_v2_chinese_char_base_new\"\n",
    "    batch_size=32\n",
    "    max_len=256\n",
    "    folds=4\n",
    "    seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c5b49eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG1.tokenizer = AutoTokenizer.from_pretrained(CFG1.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "958c1786",
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入预测结果\n",
    "np_roformer = np.load('fusion/roformer.npz')\n",
    "np_macbert = np.load('fusion/macbert.npz')\n",
    "np_roberta = np.load('fusion/roberta.npz')\n",
    "np_nezha = np.load('fusion/nezha.npz')\n",
    "list_model = [np_roformer,np_macbert,np_roberta,np_nezha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6e3f6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0]:\n",
    "#     result_prob = cp.asarray(np.zeros((sample_num,ENT_CLS_NUM,256,ner_maxlen),dtype= np.float16))\n",
    "    result_prob = [np_roformer[f'fold{i}'],np_macbert[f'fold{i}'],np_roberta[f'fold{i}'],np_nezha[f'fold{i}']]\n",
    "#     roformer_fold = np_roformer[f'fold{i}']\n",
    "#     macbert_fold = np_macbert[f'fold{i}']\n",
    "#     roberta_fold = np_roberta[f'fold{i}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1337dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_final_tokenprob = sum(result_prob)/len(result_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4f5d7eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_result = []\n",
    "# for i,value in enumerate(mean_final_tokenprob):\n",
    "#     pred = []\n",
    "#     for l,start,end in zip(*np.where(value> 0.5)):\n",
    "#         prob = value[l][start][end]\n",
    "#         pred.append((l,start, start+end,prob))\n",
    "#     pred.sort(key=lambda x: (x[1], x[2]),reverse=False)\n",
    "#     pred_result.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "897af199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成用于调阈值的文件、调阈值\n",
    "fold = 0\n",
    "pred_result = []\n",
    "for index,l,start,end in zip(*np.where(mean_final_tokenprob> 0.3)):\n",
    "    prob = mean_final_tokenprob[index][l][start][end]\n",
    "    pred_result.append((int(index),int(l),int(start), int(start+end),float(prob)))\n",
    "with open(f'fold{fold}.json',\"w\") as file_obj:\n",
    "    json.dump(pred_result,file_obj)\n",
    "\n",
    "\n",
    "\n",
    "# data = json.load(open(eval_file))\n",
    "# true_result = defaultdict(list)\n",
    "# def compute_offset_in_turn(ent,turn, original_pos):\n",
    "#     speaker1 = list(turn.keys())[0]\n",
    "#     speaker2 = list(turn.keys())[1]\n",
    "#     start, end = None, None\n",
    "#     if original_pos[0] == 1:\n",
    "#         start, end = original_pos[1:]\n",
    "#     else:\n",
    "#         start = original_pos[1] + len(turn[speaker1])\n",
    "#         end = original_pos[2] + len(turn[speaker1])\n",
    "#     if ent['type'] != '5G套餐':\n",
    "#         type_lable = type2id[ent['type']]\n",
    "#         return (start,end,type_lable)\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "# for fold in range(CFG1.folds):\n",
    "#     idx = 0\n",
    "#     for item in tqdm(data):\n",
    "#         if item['fold'] != fold:\n",
    "#             continue\n",
    "#         for turn in item[\"content\"]:\n",
    "#             text_in_turn_list = []\n",
    "#             for key in list(turn.keys())[:2]:\n",
    "#                 text_in_turn_list.append(turn[key])\n",
    "#             text_in_turn = list(\"\".join(text_in_turn_list))\n",
    "#             text_mapping = CFG1.tokenizer(text_in_turn,is_split_into_words=True)\n",
    "#             word_ids = text_mapping.word_ids()\n",
    "#             for ent in turn[\"info\"][\"ents\"]:\n",
    "#                 for position in ent[\"pos\"]:\n",
    "#                     offset = compute_offset_in_turn(ent,turn, position)\n",
    "#                     if offset==None:\n",
    "#                         continue\n",
    "#                     start_index = offset[0]\n",
    "#                     end_index =offset[1]\n",
    "#                     if text_in_turn[start_index] == ' ' or text_in_turn[start_index] == '_':\n",
    "#                         start_index += 1\n",
    "#                     if text_in_turn[end_index-1] == ' ' or text_in_turn[end_index-1] == '_':\n",
    "#                         end_index -= 1\n",
    "\n",
    "#                     token_start_index = word_ids.index(start_index)\n",
    "#                     token_end_index = len(word_ids)-1-list(reversed(word_ids)).index(end_index-1)\n",
    "#                     true_result[fold].append((idx,offset[2],token_start_index,token_end_index))\n",
    "#             idx += 1\n",
    "# with open('true_token.json',\"w\") as file_obj:\n",
    "#     json.dump(true_result,file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b66948c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_offset_in_turn(ent,turn,text_in_turn,original_pos):\n",
    "    speaker1 = list(turn.keys())[0]\n",
    "    # speaker2 = list(turn.keys())[1]\n",
    "    # start, end = None, None\n",
    "    if original_pos[0] == 1:\n",
    "        start, end = original_pos[1:]\n",
    "    else:\n",
    "        start = original_pos[1] + len(turn[speaker1])\n",
    "        end = original_pos[2] + len(turn[speaker1])\n",
    "    text1 = (\"\".join(text_in_turn[start:end])).replace(\"_\",\" \")\n",
    "    text2 = ent[\"name\"].replace(\"_\",\" \")\n",
    "    assert text1 == text2\n",
    "    if text_in_turn[start] ==' ' or text_in_turn[start] =='_':\n",
    "        start += 1\n",
    "    if text_in_turn[end-1] ==' ' or text_in_turn[end-1] =='_':\n",
    "        end -= 1\n",
    "    type_lable = type2id[ent['type']]\n",
    "    return (start,end-1,type_lable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dab2a92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 1370687.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.557485211588048 total_common:7351,total_pred:13357,total_true:13015\n"
     ]
    }
   ],
   "source": [
    "pred_char = []\n",
    "true_char = []\n",
    "idx = 0\n",
    "valid_folds = []\n",
    "#导入全部数据\n",
    "data = json.load(open(eval_file))\n",
    "\n",
    "for value in tqdm(data):\n",
    "    if value['fold'] == 3:\n",
    "        valid_folds.append(value)\n",
    "for item in valid_folds:\n",
    "    for turn in item[\"content\"]:\n",
    "        text_in_turn_list = []\n",
    "        for key in list(turn.keys())[:2]:\n",
    "            text_in_turn_list.append(turn[key])\n",
    "        text_in_turn = list(\"\".join(text_in_turn_list))\n",
    "        text_mapping = CFG1.tokenizer(text_in_turn,is_split_into_words=True)\n",
    "        word_ids = text_mapping.word_ids()\n",
    "        temp_pred = []\n",
    "        for preds_sample in pred_result[idx]:\n",
    "            lable = preds_sample[0]\n",
    "            start_char = word_ids[preds_sample[1]]\n",
    "            end_char = word_ids[preds_sample[2]]\n",
    "            prob = preds_sample[3]\n",
    "            if start_char is not None and end_char is not None:\n",
    "#                 if len(temp_pred)!=0 and start_char <= temp_pred[-1][3]:\n",
    "#                     if prob > temp_prob:\n",
    "#                         temp_pred.pop()\n",
    "#                     else:\n",
    "#                         continue\n",
    "                temp_pred.append((idx,lable,start_char,end_char))\n",
    "                temp_prob = prob\n",
    "                \n",
    "        pred_char += temp_pred\n",
    "        for ent in turn[\"info\"][\"ents\"]:\n",
    "            if ent['type'] == '5G套餐':\n",
    "                continue\n",
    "            for position in ent[\"pos\"]:\n",
    "                offset = compute_offset_in_turn(ent,turn,text_in_turn,position)\n",
    "                true_char.append((idx,offset[2],offset[0],offset[1]))\n",
    "        idx += 1\n",
    "R = set(pred_char)\n",
    "T = set(true_char)\n",
    "total_common = len(R & T)\n",
    "total_pred = len(R)\n",
    "total_true = len(T)\n",
    "f1, precision, recall = 2 * total_common / (total_pred + total_true), total_common / total_pred, total_common / total_true\n",
    "print(f1,\"total_common:{},total_pred:{},total_true:{}\".format(total_common, total_pred,total_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a67a70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
